{
  
    
        "post0": {
            "title": "Titanic: Machine Learning from Disaster",
            "content": "Overview . . Description . The sinking of the Titanic is one of the most infamous shipwrecks in history. . On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. . While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. . In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc). . What Data Will I Use in This Competition? . In this competition, you’ll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled train.csv and the other is titled test.csv. . Train.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the “ground truth”. . The test.csv dataset contains similar information but does not disclose the “ground truth” for each passenger. It’s your job to predict these outcomes. . Using the patterns you find in the train.csv data, predict whether the other 418 passengers on board (found in test.csv) survived. . Evaluation . Goal . It is your job to predict if a passenger survived the sinking of the Titanic or not. For each in the test set, you must predict a 0 or 1 value for the variable. . Metric . Your score is the percentage of passengers you correctly predict. This is known as accuracy. . Submission File Format . You should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows. . The file should have exactly 2 columns: . PassengerId (sorted in any order) Survived (contains your binary predictions: 1 for survived, 0 for deceased) PassengerId,Survived 892,0 893,1 894,0 Etc. . Problem-Solving Process . . Important: Although the problem-solving process presented here looks like a linear and waterfall style, it&#8217;s actually really an iterative process, which means that you may need to go back and forth to make sure your previous hypothesis was correct, or you need to test whether your new idea really works. . Initial Setup . First we need to import libraries . #collapse-hide %matplotlib inline import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np import collections # allow IPython Notebook cell multiple outputs from IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = &quot;all&quot; from sklearn.impute import SimpleImputer from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split import csv import warnings warnings.filterwarnings(&#39;ignore&#39;) . . Initialize plotting parameters . #collapse-hide plt.rcParams[&#39;figure.figsize&#39;] = (12,8) plt.rcParams[&#39;font.size&#39;] = 14 plt.rcParams[&#39;axes.grid&#39;] = True . . Helper functions . #collapse-hide def set_seaborn_plot_style(ax, xlabel, ylabel, font_size=14): ax.legend(loc=&#39;upper right&#39;, fontsize=14); ax.tick_params(axis=&quot;x&quot;, labelsize=font_size); ax.tick_params(axis=&quot;y&quot;, labelsize=font_size); ax.set_xlabel(xlabel, fontsize=font_size); ax.set_ylabel(ylabel, fontsize=font_size); . . Then we download training/testing data from Kaggle using kaggle-api. . #collapse-hide !kaggle competitions download -c titanic --force !unzip -o titanic.zip -d ./datasets . . Downloading titanic.zip to /Users/cyyeh/Desktop/TYCS/blog/_notebooks 0%| | 0.00/34.1k [00:00&lt;?, ?B/s] 100%|██████████████████████████████████████| 34.1k/34.1k [00:00&lt;00:00, 5.36MB/s] Archive: titanic.zip inflating: ./datasets/gender_submission.csv inflating: ./datasets/test.csv inflating: ./datasets/train.csv . . Note: If you see an error like this, IOError: Could not find kaggle.json. Make sure it is located in /root/.kaggle. Or use the environment method., please upload your kagggle.json file which has api username and api key inside. . Let&#39;s check what are inside the datasets directory . #collapse-hide !ls ./datasets . . gender_submission.csv test.csv train.csv . EDA . A Sneak Peek . Let&#39;s load data into pandas DataFrame format, since it&#39;s much easier for computation and anlysis. Then we can have a sneak peek at the datasets. . #collapse-hide train_data_path = &#39;./datasets/train.csv&#39; test_data_path = &#39;./datasets/test.csv&#39; train_df = pd.read_csv(train_data_path) test_df = pd.read_csv(test_data_path) print(&quot;Training dataset&quot;) train_df print(&quot;Testing dataset&quot;) test_df . . Training dataset . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 887 | 0 | 2 | Montvila, Rev. Juozas | male | 27.0 | 0 | 0 | 211536 | 13.0000 | NaN | S | . 887 888 | 1 | 1 | Graham, Miss. Margaret Edith | female | 19.0 | 0 | 0 | 112053 | 30.0000 | B42 | S | . 888 889 | 0 | 3 | Johnston, Miss. Catherine Helen &quot;Carrie&quot; | female | NaN | 1 | 2 | W./C. 6607 | 23.4500 | NaN | S | . 889 890 | 1 | 1 | Behr, Mr. Karl Howell | male | 26.0 | 0 | 0 | 111369 | 30.0000 | C148 | C | . 890 891 | 0 | 3 | Dooley, Mr. Patrick | male | 32.0 | 0 | 0 | 370376 | 7.7500 | NaN | Q | . 891 rows × 12 columns . Testing dataset . PassengerId Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 892 | 3 | Kelly, Mr. James | male | 34.5 | 0 | 0 | 330911 | 7.8292 | NaN | Q | . 1 893 | 3 | Wilkes, Mrs. James (Ellen Needs) | female | 47.0 | 1 | 0 | 363272 | 7.0000 | NaN | S | . 2 894 | 2 | Myles, Mr. Thomas Francis | male | 62.0 | 0 | 0 | 240276 | 9.6875 | NaN | Q | . 3 895 | 3 | Wirz, Mr. Albert | male | 27.0 | 0 | 0 | 315154 | 8.6625 | NaN | S | . 4 896 | 3 | Hirvonen, Mrs. Alexander (Helga E Lindqvist) | female | 22.0 | 1 | 1 | 3101298 | 12.2875 | NaN | S | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 413 1305 | 3 | Spector, Mr. Woolf | male | NaN | 0 | 0 | A.5. 3236 | 8.0500 | NaN | S | . 414 1306 | 1 | Oliva y Ocana, Dona. Fermina | female | 39.0 | 0 | 0 | PC 17758 | 108.9000 | C105 | C | . 415 1307 | 3 | Saether, Mr. Simon Sivertsen | male | 38.5 | 0 | 0 | SOTON/O.Q. 3101262 | 7.2500 | NaN | S | . 416 1308 | 3 | Ware, Mr. Frederick | male | NaN | 0 | 0 | 359309 | 8.0500 | NaN | S | . 417 1309 | 3 | Peter, Master. Michael J | male | NaN | 1 | 1 | 2668 | 22.3583 | NaN | C | . 418 rows × 11 columns . So, for the training dataset, there are 891 instances; while for the testing dataset, there are only 418 instances. Besides that, we can see that the training dataset has 12 columns(11 feature columns), since it has a label(Survived) for each instance. Now let&#39;s understand the training dataset further, such as inspect data type of each column, check if there is any null value, etc. . More Information on Data: Data Types, Missing Values, etc. . #collapse-hide print(&#39;training data&#39;) train_df.info() print() print(&#39;tesitng data&#39;) test_df.info() . . training data &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB tesitng data &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 418 entries, 0 to 417 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 418 non-null int64 1 Pclass 418 non-null int64 2 Name 418 non-null object 3 Sex 418 non-null object 4 Age 332 non-null float64 5 SibSp 418 non-null int64 6 Parch 418 non-null int64 7 Ticket 418 non-null object 8 Fare 417 non-null float64 9 Cabin 91 non-null object 10 Embarked 418 non-null object dtypes: float64(2), int64(4), object(5) memory usage: 36.0+ KB . Since there are 891 entries in total, so Age, Cabin and Embarked columns have missing values(null). Besides, since this is a binary classification task, we need to make sure the labels really have two values only.(0 or 1) . #collapse-hide train_df[&#39;Survived&#39;].value_counts() . . 0 549 1 342 Name: Survived, dtype: int64 . Now let&#39;s understand the meaning of each column in traning data deeper. . Data Dictionary . Variable Definition Key . survival | Survivial | 0 = No, 1 = Yes | . pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd | . sex | Sex | | . Age | Age in years | | . sibsp | # of siblings/spouses aboard the Titanic | | . parch | # of parents/children aboard the Titanic | | . ticket | Ticket number | | . fare | Passenger fare | | . cabin | Cabin number | | . embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton | . Variable Notes . Tip: Since the numbers below all have true meanings, so it&#8217;s better to simply use regular numbers for encoding the features. If a feature doesn&#8217;t have ordinal characteristics, then we can transfer numbers to the one-hot encoding format. . pclass: A proxy for socio-economic status (SES) . 1st = Upper | 2nd = Middle | 3rd = Lower | . age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5 . sibsp: The dataset defines family relations in this way... . Sibling = brother, sister, stepbrother, stepsister | Spouse = husband, wife (mistresses and fiancés were ignored) | . parch: The dataset defines family relations in this way... . Parent = mother, father | Child = daughter, son, stepdaughter, stepson Some children travelled only with a nanny, therefore parch=0 for them. | . | . Plotting for Easier Exploration . PassengerId . #collapse-hide plt.title(&#39;PassengerId&#39;); plt.xlabel(&#39;Instance Entry&#39;); plt.ylabel(&#39;PassengerId&#39;); plt.plot(train_df[&#39;PassengerId&#39;]); . . It should be a straight line . #collapse-hide assert collections.Counter(train_df[&#39;PassengerId&#39;]) == collections.Counter(range(1, 892)), &#39;PassengerId is not a straight line&#39; . . Pclass . #collapse-hide plt.title(&#39;Ticket class(1=1st, 2=2nd, 3=3rd)&#39;); plt.xlabel(&#39;Pclass&#39;); plt.ylabel(&#39;Number&#39;); plt.hist(train_df[&#39;Pclass&#39;]); . . Sex . #collapse-hide plt.title(&#39;Sex&#39;); plt.xlabel(&#39;Sex&#39;); plt.ylabel(&#39;Number&#39;); plt.hist(train_df[&#39;Sex&#39;]); . . Age . #collapse-hide plt.title(&#39;Age in years&#39;); plt.xlabel(&#39;Age&#39;); plt.ylabel(&#39;Number&#39;); plt.hist(train_df[&#39;Age&#39;].dropna(), bins=30); . . . Note: The Age column has 177(891-714) null values. . Sibsp . #collapse-hide plt.title(&#39;# of siblings/spouses aboard the Titanic&#39;); plt.xlabel(&#39;SibSp&#39;); plt.ylabel(&#39;Number&#39;); plt.hist(train_df[&#39;SibSp&#39;]); . . Parch . #collapse-hide plt.title(&#39;# of parents/children aboard the Titanic&#39;); plt.xlabel(&#39;Parch&#39;); plt.ylabel(&#39;Number&#39;); plt.hist(train_df[&#39;Parch&#39;]); . . Fare . #collapse-hide plt.title(&#39;Passenger fare&#39;); plt.xlabel(&#39;Fare&#39;); plt.ylabel(&#39;Number&#39;); plt.hist(train_df[&#39;Fare&#39;], bins=30); . . Embarked . #collapse-hide plt.title(&#39;Port of Embarkation(C=Cherbourg, Q=Queenstown, S=Southampton)&#39;); plt.xlabel(&#39;Embarked&#39;); plt.ylabel(&#39;Number&#39;); plt.hist(train_df[&#39;Embarked&#39;].dropna()); . . Correlation Plot . After we inspect distributions of each feature column, now we are going to see the correlation plot among features and the label. . #collapse-hide plt.figure(figsize=(12, 12)); ax = sns.heatmap(train_df.corr(), annot=True, center=0); ax.set_yticklabels(ax.get_yticklabels(), rotation = 0); . . . Note: The Survived column is the label of the training dataset. . We can easily observe that no two columns are highly correlated with each other(like &gt;0.7). Also, it makes sense that some features are a lot more correlated with some other features. For example, Pclass and Survived and Pclass and Fare. Now I want to further draw plots to understand more among these &quot;highly-correlated&quot; columns. . Scatter Plots . Pclass vs. Fare with Survived as hue . #collapse-hide xlabel = &#39;Pclass&#39; ylabel = &#39;Fare with log10 scale&#39; plt.figure(figsize=(12, 8)); ax = sns.scatterplot(x=train_df[&#39;Pclass&#39;], y=np.log10(train_df[&#39;Fare&#39;]), hue=train_df[&#39;Survived&#39;]); set_seaborn_plot_style(ax, xlabel, ylabel); . . We can easily see that fare is much higher in the 1st class. However, there seems like an outlier point(blue) in the bottom left that we may need to investigate further. . Pclass vs. Age with Survived as hue . #collapse-hide xlabel = &#39;Pclass&#39; ylabel = &#39;Age&#39; plt.figure(figsize=(12, 8)); ax = sns.scatterplot(x=&#39;Pclass&#39;, y=&#39;Age&#39;, hue=&#39;Survived&#39;, data=train_df); set_seaborn_plot_style(ax, xlabel, ylabel); . . We can at least reach these conclusions: . Most people who were survived are in the 1st class, and least people in the 3rd class were survived | For survivers among all classes, the 1st class has the biggest spread | For the 2nd and 3rd classes, most survivers are in the younger ages, especially for the 2nd class | . Age vs. Fare with Survived as hue . #collapse-hide xlabel=&#39;Age&#39; ylabel=&#39;Fare&#39; plt.figure(figsize=(12, 8)); ax = sns.scatterplot(x=&#39;Age&#39;, y=&#39;Fare&#39;, hue=&#39;Survived&#39;, data=train_df); set_seaborn_plot_style(ax, xlabel, ylabel); . . We can easily observe that more adults who had higher fare were survived than those who had lower fare. . Feature Engineering . Since machine learning algorithms only take numerical data, so we need to make sure every feature has appropriate form before modeling. This process is called feature engineering, including some common operations like dealing with missing values, choosing appropriate features to feed into models, some mathematical transformation on features, encoding categorical feature columns, etc. . . Tip: What operations to take in feature engineering plays an important role in influencing model performance, so we need to choose appropriate operations based on what model we decide to use. . Choose appropriate features . There are two columns(Name, Ticket) with text features, and it&#39;s more complicated. So, I&#39;ll discard them first for the baseline model. The PassengerId column doesn&#39;t correlate with other featrues, I&#39;ll discard them as they look like doesn&#39;t have the true meaning. . #collapse-hide label = [&#39;Survived&#39;] features = [&#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;] train_df_ = train_df[features+label] test_df_ = test_df[features] print(&quot;training data&quot;) train_df_ print(&quot;testing data&quot;) test_df_ . . training data . Pclass Sex Age SibSp Parch Fare Cabin Embarked Survived . 0 3 | male | 22.0 | 1 | 0 | 7.2500 | NaN | S | 0 | . 1 1 | female | 38.0 | 1 | 0 | 71.2833 | C85 | C | 1 | . 2 3 | female | 26.0 | 0 | 0 | 7.9250 | NaN | S | 1 | . 3 1 | female | 35.0 | 1 | 0 | 53.1000 | C123 | S | 1 | . 4 3 | male | 35.0 | 0 | 0 | 8.0500 | NaN | S | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 2 | male | 27.0 | 0 | 0 | 13.0000 | NaN | S | 0 | . 887 1 | female | 19.0 | 0 | 0 | 30.0000 | B42 | S | 1 | . 888 3 | female | NaN | 1 | 2 | 23.4500 | NaN | S | 0 | . 889 1 | male | 26.0 | 0 | 0 | 30.0000 | C148 | C | 1 | . 890 3 | male | 32.0 | 0 | 0 | 7.7500 | NaN | Q | 0 | . 891 rows × 9 columns . testing data . Pclass Sex Age SibSp Parch Fare Cabin Embarked . 0 3 | male | 34.5 | 0 | 0 | 7.8292 | NaN | Q | . 1 3 | female | 47.0 | 1 | 0 | 7.0000 | NaN | S | . 2 2 | male | 62.0 | 0 | 0 | 9.6875 | NaN | Q | . 3 3 | male | 27.0 | 0 | 0 | 8.6625 | NaN | S | . 4 3 | female | 22.0 | 1 | 1 | 12.2875 | NaN | S | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 413 3 | male | NaN | 0 | 0 | 8.0500 | NaN | S | . 414 1 | female | 39.0 | 0 | 0 | 108.9000 | C105 | C | . 415 3 | male | 38.5 | 0 | 0 | 7.2500 | NaN | S | . 416 3 | male | NaN | 0 | 0 | 8.0500 | NaN | S | . 417 3 | male | NaN | 1 | 1 | 22.3583 | NaN | C | . 418 rows × 8 columns . Dealing with missing values . Discard the column(if there are too many null values, or some critical information is missing) | Discard the row(but this would make less training data) | Imputation(with the mean, median, min, max, etc. value of other instances) | . We inspect what columns have missing values first . #collapse-hide print(&quot;training data&quot;) train_df_.info() print() print(&quot;testing data&quot;) test_df_.info() . . training data &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 Pclass 891 non-null int64 1 Sex 891 non-null object 2 Age 714 non-null float64 3 SibSp 891 non-null int64 4 Parch 891 non-null int64 5 Fare 891 non-null float64 6 Cabin 204 non-null object 7 Embarked 889 non-null object 8 Survived 891 non-null int64 dtypes: float64(2), int64(4), object(3) memory usage: 62.8+ KB testing data &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 418 entries, 0 to 417 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 Pclass 418 non-null int64 1 Sex 418 non-null object 2 Age 332 non-null float64 3 SibSp 418 non-null int64 4 Parch 418 non-null int64 5 Fare 417 non-null float64 6 Cabin 91 non-null object 7 Embarked 418 non-null object dtypes: float64(2), int64(3), object(3) memory usage: 26.2+ KB . We can see that the Cabin column has too many null values both in training and testing data, so I&#39;ll discard it. . #collapse-hide label = [&#39;Survived&#39;] features = [&#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Embarked&#39;] train_df_ = train_df[features+label] test_df_ = test_df[features] print(&quot;training data&quot;) train_df_ print(&quot;testing data&quot;) test_df_ . . training data . Pclass Sex Age SibSp Parch Fare Embarked Survived . 0 3 | male | 22.0 | 1 | 0 | 7.2500 | S | 0 | . 1 1 | female | 38.0 | 1 | 0 | 71.2833 | C | 1 | . 2 3 | female | 26.0 | 0 | 0 | 7.9250 | S | 1 | . 3 1 | female | 35.0 | 1 | 0 | 53.1000 | S | 1 | . 4 3 | male | 35.0 | 0 | 0 | 8.0500 | S | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 886 2 | male | 27.0 | 0 | 0 | 13.0000 | S | 0 | . 887 1 | female | 19.0 | 0 | 0 | 30.0000 | S | 1 | . 888 3 | female | NaN | 1 | 2 | 23.4500 | S | 0 | . 889 1 | male | 26.0 | 0 | 0 | 30.0000 | C | 1 | . 890 3 | male | 32.0 | 0 | 0 | 7.7500 | Q | 0 | . 891 rows × 8 columns . testing data . Pclass Sex Age SibSp Parch Fare Embarked . 0 3 | male | 34.5 | 0 | 0 | 7.8292 | Q | . 1 3 | female | 47.0 | 1 | 0 | 7.0000 | S | . 2 2 | male | 62.0 | 0 | 0 | 9.6875 | Q | . 3 3 | male | 27.0 | 0 | 0 | 8.6625 | S | . 4 3 | female | 22.0 | 1 | 1 | 12.2875 | S | . ... ... | ... | ... | ... | ... | ... | ... | . 413 3 | male | NaN | 0 | 0 | 8.0500 | S | . 414 1 | female | 39.0 | 0 | 0 | 108.9000 | C | . 415 3 | male | 38.5 | 0 | 0 | 7.2500 | S | . 416 3 | male | NaN | 0 | 0 | 8.0500 | S | . 417 3 | male | NaN | 1 | 1 | 22.3583 | C | . 418 rows × 7 columns . For other columns that have missing values, We can use choose appropriate imputation techniques based on their distribution. Now, we simply choose the most_frequent strategy for brevity. Finally we make sure they don&#39;t have any missing values. . #collapse-hide label = [&#39;Survived&#39;] features = [&#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Embarked&#39;] imp = SimpleImputer(missing_values=np.nan, strategy=&#39;most_frequent&#39;) dtypes = train_df_.dtypes train_df_impute = pd.DataFrame(imp.fit_transform(train_df_[features]), columns=features) train_df_ = pd.concat([train_df_impute, train_df_[label]], axis=1) test_df_ = pd.DataFrame(imp.transform(test_df_), columns=features) for i, feature in enumerate(features): train_df_[feature] = train_df_[feature].astype(dtypes[i]) test_df_[feature] = test_df_[feature].astype(dtypes[i]) print(&quot;training data&quot;) train_df_.info() print() print(&quot;testing data&quot;) test_df_.info() . . training data &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 Pclass 891 non-null int64 1 Sex 891 non-null object 2 Age 891 non-null float64 3 SibSp 891 non-null int64 4 Parch 891 non-null int64 5 Fare 891 non-null float64 6 Embarked 891 non-null object 7 Survived 891 non-null int64 dtypes: float64(2), int64(4), object(2) memory usage: 55.8+ KB testing data &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 418 entries, 0 to 417 Data columns (total 7 columns): # Column Non-Null Count Dtype -- -- 0 Pclass 418 non-null int64 1 Sex 418 non-null object 2 Age 418 non-null float64 3 SibSp 418 non-null int64 4 Parch 418 non-null int64 5 Fare 418 non-null float64 6 Embarked 418 non-null object dtypes: float64(2), int64(3), object(2) memory usage: 23.0+ KB . Some mathematical transformations on features . Normalization(make feature values between [0, 1]) | Standardization(with mean 0, vairance 1) | Log-transformation(make feature values with smaller variation) | . This part plays an important role in model performance, and it really depends on what model you choose. For example, if your model uses the &quot;distance-based&quot; algorithm like SGD or ridge regression, then standardization may help. . Encoding categorical feature columns . One-hot encoding(if the column values don&#39;t have meaningful ordinal information) | Ordinal numbers | . It seems like the Embarked column should also take one-hot encoding format; however, I am curious if they actually correlate with other columns. If they are correlated with other columns, it makes more sense to have ordinal characteristics. . #collapse_hide embarked_mapping = { &#39;S&#39;: 1, &#39;C&#39;: 2, &#39;Q&#39;: 3 } train_df_[&#39;Embarked&#39;] = train_df_[&#39;Embarked&#39;].map(embarked_mapping) test_df_[&#39;Embarked&#39;] = test_df_[&#39;Embarked&#39;].map(embarked_mapping) . . Let&#39;s plot a correlation plot and pay attention to relation between the Embarked column with other columns! . #collapse_hide plt.figure(figsize=(12, 12)); ax = sns.heatmap(train_df_.corr(), annot=True, center=0); ax.set_yticklabels(ax.get_yticklabels(), rotation = 0); . . Since the Embarked column is somewhat correlated with the Survived column, so I think it makes sense that I transform the column as ordinal format. . Finally, the Sex column should&#39;t have ordinal characteristics, so we&#39;ll transform it to be one-hot encoding format. . #collapse_hide train_df_ = pd.get_dummies(train_df_) test_df_ = pd.get_dummies(test_df_) print(&#39;training data&#39;) train_df_ print() print(&#39;testing data&#39;) test_df_ . . training data . Pclass Age SibSp Parch Fare Embarked Survived Sex_female Sex_male . 0 3 | 22.0 | 1 | 0 | 7.2500 | 1 | 0 | 0 | 1 | . 1 1 | 38.0 | 1 | 0 | 71.2833 | 2 | 1 | 1 | 0 | . 2 3 | 26.0 | 0 | 0 | 7.9250 | 1 | 1 | 1 | 0 | . 3 1 | 35.0 | 1 | 0 | 53.1000 | 1 | 1 | 1 | 0 | . 4 3 | 35.0 | 0 | 0 | 8.0500 | 1 | 0 | 0 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 2 | 27.0 | 0 | 0 | 13.0000 | 1 | 0 | 0 | 1 | . 887 1 | 19.0 | 0 | 0 | 30.0000 | 1 | 1 | 1 | 0 | . 888 3 | 24.0 | 1 | 2 | 23.4500 | 1 | 0 | 1 | 0 | . 889 1 | 26.0 | 0 | 0 | 30.0000 | 2 | 1 | 0 | 1 | . 890 3 | 32.0 | 0 | 0 | 7.7500 | 3 | 0 | 0 | 1 | . 891 rows × 9 columns . testing data . Pclass Age SibSp Parch Fare Embarked Sex_female Sex_male . 0 3 | 34.5 | 0 | 0 | 7.8292 | 3 | 0 | 1 | . 1 3 | 47.0 | 1 | 0 | 7.0000 | 1 | 1 | 0 | . 2 2 | 62.0 | 0 | 0 | 9.6875 | 3 | 0 | 1 | . 3 3 | 27.0 | 0 | 0 | 8.6625 | 1 | 0 | 1 | . 4 3 | 22.0 | 1 | 1 | 12.2875 | 1 | 1 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 413 3 | 24.0 | 0 | 0 | 8.0500 | 1 | 0 | 1 | . 414 1 | 39.0 | 0 | 0 | 108.9000 | 2 | 1 | 0 | . 415 3 | 38.5 | 0 | 0 | 7.2500 | 1 | 0 | 1 | . 416 3 | 24.0 | 0 | 0 | 8.0500 | 1 | 0 | 1 | . 417 3 | 24.0 | 1 | 1 | 22.3583 | 2 | 0 | 1 | . 418 rows × 8 columns . Now we finally have data ready to reach to the most exciting modeling part! . Modeling and Evaluation . Baseline Model . After having some time doing data exploration, we are now going to build a model and evaluate its performance. The goal of building the first model is not to have the best performance; rather, we would want a baseline that can help us understand where and how we can do better. After all, we need to understand the problem-solving process should be iterative! . In order to build a machine learning model, we need several things prepared: . Model-ready data: it should be data after doing feature engineering, not just raw data | Machine learning algorithm: this is what modeling stands for | Evaluation Metric: in this problem, we are given to use &quot;accuracy&quot; as the metric | Since it&#39;s a binary classification task, I&#39;ll choose logistic regression as the baseline model! . Use the cross-validation approach to evaluate model performance . #collapse-hide features = test_df_.columns label = [&#39;Survived&#39;] # random_state: for reproducibility X_train, X_test, y_train, y_test = train_test_split(train_df_[features], train_df_[label], test_size=0.2, random_state=0) print(f&quot;X_train shape: {X_train.shape}, y_train shape: {y_train.shape}&quot;) print(f&quot;X_test shape: {X_test.shape}, y_test shape: {y_test.shape}&quot;) . . X_train shape: (712, 8), y_train shape: (712, 1) X_test shape: (179, 8), y_test shape: (179, 1) . Model training . #collapse-hide clf = LogisticRegression().fit(X_train, y_train) print(f&quot;training accuracy: {clf.score(X_test, y_test)}&quot;) . . accuracy: 0.8212290502793296 . Our baseline model reaches 82% accuracy! Not bad! Then we can submit prediction results on real testing data to Kaggle to check the true performance. . Model prediction . #collapse-hide survived = clf.predict(test_df_) . . Save the Model Prediction Results to CSV . #collapse-hide submission_dict = { &#39;PassengerId&#39;: [*range(892, 1310)], &#39;Survived&#39;: clf.predict(test_df_) } # sanity check assert len(submission_dict[&#39;PassengerId&#39;]) == len(submission_dict[&#39;Survived&#39;]), f&quot;PasserngerId values({len(submission_dict[&#39;PassengerId&#39;])}) length should equal to Survived values({len(submission_dict[&#39;Survived&#39;])}) length&quot; # save the dict to csv file with open(&#39;gender_submission.csv&#39;, &#39;w&#39;) as csvfile: fieldnames = submission_dict.keys() writer = csv.DictWriter(csvfile, fieldnames=fieldnames) writer.writeheader() for passenger_id, survived in zip(submission_dict[&#39;PassengerId&#39;], submission_dict[&#39;Survived&#39;]): _ = writer.writerow({&#39;PassengerId&#39;: passenger_id, &#39;Survived&#39;: survived}) #depress output . . Submit to Kaggle . #collapse-hide !kaggle competitions submit -f gender_submission.csv -m &quot;submit baseline model&quot; titanic . . 100%|████████████████████████████████████████| 3.18k/3.18k [00:05&lt;00:00, 647B/s] Successfully submitted to Titanic: Machine Learning from Disaster . . Although the training accuracy reaches 82%, the testing accuracy only has 75%. There might be overfitting or we need to use a better model! . Reference . Titanic: Machine Learning from Disaster | . Comments .",
            "url": "https://data-products.info/kaggle-titanic.html",
            "relUrl": "/kaggle-titanic.html",
            "date": " • Mar 31, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "You can know more about me from here. .",
          "url": "https://data-products.info/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}